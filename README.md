# CustomDB

------

CustomDB is a key-value storage library and it use both [HashChain](http://en.wikipedia.org/wiki/Hash_chain) and [Extendible_hashing](http://en.wikipedia.org/wiki/Extendible_hashing) as internel structure. Also, I have implemented LRU-Cache and FIFO-Cache in its external Cache-System. Anyone, who has interest in it, can add self-customed structure or Cache-Schedule Algorithm to enhance its ability. That is the reason I renamed it as `CustomDB`.

Our work is mainly inspired by [APUE](www.apuebook.com), [GDBM](www.gnu.org/s/gdbm/), [leveldb](https://code.google.com/p/leveldb/) and [yodb](https://github.com/kedebug/yodb). Thanks again for open-source group.

## Features

> * Keys and values are arbitrary byte arrays.
> * The basic operations are Put(key,value), Get(key), Delete(key).
> * Custom-option Ability, like turn on/off log. As limitation of time, I don't provide enough implementation for different option. Any one who has interest in it can try it. Very simple.
> * Exposed DBImpl(database implementation) interface and Cache interface.
> * Some blogs of this project(under construction) about how to use the library and the design principle of this library.
> * Little RAM requirment.
> * Small Index requirment(Compared with leveldb).

## Limitations

> * This is not a SQL database. It does not have a relational data model, it does not support SQL queries, and it has no support for indexes.
> * Only a single process can access a particular database at a time. However, multiple-threading can be accepted. For the detail, read db_batch_thread.cpp and db_parallel_thread.cpp in detail.
> * Only localhost db is supported.
> * I doesn't provide any compression tool in this db. Therefore, when the etries in very large(like 10 Million and each entry has 100 bytes), the space requirement is very large.
> * It only supports random write and random read(As limitation of HashTable).

## Performance
To test our library, I used the test suit of leveldb and succeed to test CustomDB's performance.s

### 1. Setup
We tried our database with a million entries. Each entry has a 16 byte key, and a 100 byte value. Too get maxinum performance, we turned off log and cache(As the space of the key is far larger than we put, the cache has little performance balance.)

```
CustomDB:   version 1.0
Date:       Wed Feb 19 21:55:09 2014
CPU:        4 * Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz
CPUCache:   3072 KB
Keys:       16 bytes each
Values:     100 bytes each
Entries:    1000000
RawSize:    150.0 MB (estimated)
FileSize:   116.0 MB (estimated)
WARNING:    Log is disabled, for better benchmarks
WARNING:    Cache is disabled, for better benchmarks.
```

### 2. Write performance
The "fill" benchmarks create a brand new database, in random order. The "fillsync" benchmark flushes data from the operating system to the disk after every operation; the other write operations leave the data sitting in the operating system buffer cache for a while.

```
fillsync        :       55.0 us/op;   2.01 MB/s     
fillbatch       :       10.0 us/op;   16.1 MB/s 
fillthreadbatch :       18.0 us/op;   8.88 MB/s
fillthreadbatch :       21.0 us/op;   7.61 MB/s
```
Each "op" above corresponds to a read/write of a single key/value pair. 
### 3. Read performance
We list the performance of a random lookup. Note that the database created by the benchmark is quite small. Therefore the report characterizes the performance of leveldb when the working set fits in memory. Before this test, I have clear the cache generated by put operation.
```
readrandom      :       19.0 us/op;   8.42 MB/s  
```

### 4. Compression Ration
CustomDB provide compact feature to compression raw database. However, it only can compress fillsync, which is the most application situation. That can be called time-memory trade-off technology.
```
            Before Compression  |  After Compression
Raw Size:   230 MB                   131 MB
```

## Usage
I just tested it on Ubuntu 13.10, so if you have any problem, please feel free to send email to me.

### 1. Compile and Installation
CustomDB use GNU Make to handle compilation, you can find detail information from [this url](www.gnu.com/Make/).
```
$ make
$ make tests
$ make db_tests
$ sudo make install
```

### 2. Detail Usage
#### 2.1 Open & Close CustomDB

```c++
   Option option; //Detail? Read Source Code. 
   CustomDB db = new CustomDB;
   db -> init(option);  
   db -> close();
   delete db;
```

#### 2.2 Dump Into Terminal

```c++
   Option option;
   CustomDB db = new CustomDB;
   db -> init(option);  
   db -> dump();
   db -> close();
   delete db;
```
#### 2.3 Put Elementse

```c++
   Slice key("slice"); //Slice is default method to manpiate the item in the database.
   Slice value("123213");
   db -> put(key, value); // Return value will give out whether we put it successfully.
   db -> put("123213","iipppp"); //Another way to Put elements in it.
```

#### 2.4 Get Elements

```c++
   Slice key("slice"); 

   Slice value = db -> get(key); 
   assert(value.size() != 0); //Not empty.
```

#### 2.5 Remove Elements

```c++
   Slice key("slice");
   bool successful = db -> remove(key);
```

#### 2.6 Batch Write

```c++
   WriteBatch batch(3);
   batch.put(k1, k2);
   batch.put(k2, k3);
   batch.put(k3, k1);
   db -> write(&batch);
```

#### 2.7 Multiple_Threading Writting

```c++
   WriteBatch batch(3);
   batch.put(k1, k2);
   batch.put(k2, k3);
   batch.put(k3, k1);
   db -> write(&batch);
   db -> runBatchParallel(&batch);
```
![CustomDB UML Graph](https://raw.githubusercontent.com/mathewes/blog-dot-file/master/CustomDB.png)

## Todo List
> * Test Module
> * Parallel Map(like concurrentmap)
> * Comment
> * Backend Job